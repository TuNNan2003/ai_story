# Grandma 后端

一个基于 Go 和 Gin 框架的智能对话系统后端，支持多模型接入、流式响应和 RAG（检索增强生成）功能，特别针对长篇故事创作场景进行了优化。

## 📋 目录

- [特性](#特性)
- [技术架构](#技术架构)
- [核心功能实现](#核心功能实现)
- [技术难点与解决方案](#技术难点与解决方案)
- [快速开始](#快速开始)
- [API 文档](#api-文档)
- [配置说明](#配置说明)
- [开发指南](#开发指南)

## ✨ 特性

Grandma 后端系统提供了全面的智能对话能力，通过统一接口支持 OpenAI 和 Anthropic 等多种大语言模型，实现了实时流式返回 AI 响应的能力，为用户提供更好的交互体验。系统核心采用了基于向量检索的上下文增强技术（RAG），能够显著提升 AI 回答的准确性和一致性，特别是在需要保持长期记忆和上下文连贯性的场景中。

系统设计支持两种工作模式，以适应不同的使用场景。普通对话模式适用于日常对话场景，提供了简洁的对话历史管理和标题生成功能。而灵感模式则专门为长篇故事创作进行了深度优化，不仅支持多文档管理，还通过 RAG 技术实现了人物设定、世界观和情节的一致性维护，确保创作过程中新内容与已有内容保持高度一致。

在数据持久化方面，系统基于 SQLite 实现了轻量级的数据存储方案，支持完整的对话历史和文档管理功能，同时通过用户 ID 实现了完整的多用户数据隔离机制，确保不同用户之间的数据完全隔离。

在技术实现上，系统采用了多项优化策略。RAG 索引采用异步处理机制，所有索引操作都在独立的 goroutine 中执行，不会阻塞主流程，从而保证了系统的响应速度。文本切片服务支持段落优先和固定大小两种策略，优先按段落分割以保持语义完整性，当段落过长时自动切换到固定大小分割，并且在 chunk 之间保留重叠区域以避免语义边界丢失。检索结果结合了时间衰减因子，较新的内容会被赋予更高的权重，确保系统优先使用最新的相关信息。流式响应过程中实现了实时保存到数据库的机制，通过缓冲批量更新策略，既保证了数据的完整性，又避免了频繁 I/O 操作对性能的影响。针对不同的使用场景，系统还实现了差异化的上下文构建策略，普通对话模式使用简洁的背景信息提示，而灵感模式则构建包含详细分类信息的系统提示，将检索到的信息按人物设定、世界观设定、已有情节等类型进行组织。

## 🏗️ 技术架构

系统采用经典的分层架构设计，从 HTTP 层开始，通过 Gin 路由器和 CORS 中间件处理所有外部请求。请求经过 Handler 层进行参数验证和响应格式化，Handler 层包含了聊天、对话、文档、创作和故事等多个处理器。业务逻辑集中在 Service 层处理，包括聊天服务、RAG 服务以及其他业务服务。Service 层之下是 Repository 层，负责数据访问抽象，封装了所有数据库操作。最底层是 Model 层，定义了所有业务实体和请求响应结构。

整个系统通过这种分层设计实现了清晰的职责分离，每一层都专注于自己的核心功能，上层依赖下层提供的接口，而下层不依赖上层，这种设计使得系统具有良好的可维护性和可扩展性。当需要添加新功能时，只需要在相应的层次添加新的组件，而不会影响其他部分的实现。

在模块组织上，系统按照功能域进行了清晰的划分。配置管理模块负责加载和解析环境变量，数据库模块处理数据库的初始化和自动迁移。数据模型模块定义了所有业务实体，包括对话、文档、创作、故事和向量 chunk 等。数据访问层通过 Repository 模式封装了所有数据库操作，每个业务实体都有对应的 Repository。基础服务层提供了模型提供者、Embedding、文本切片和向量存储等通用服务。业务模块层包含了聊天、对话管理、文档管理、创作管理、故事管理和 RAG 服务等核心业务逻辑。工具函数模块提供了 ID 生成和哈希计算等通用工具。

## 🔧 核心功能实现

### 流式响应处理

流式响应处理是系统的核心功能之一，它通过实现 `io.Writer` 接口的 `responseCollector` 结构来收集和转发数据。当大模型 API 返回流式数据时，`responseCollector` 的 `Write` 方法会被调用，此时系统需要同时完成两个任务：将数据实时转发给客户端以提供良好的用户体验，以及将数据保存到数据库以确保数据不丢失。

为了实现这两个目标，系统采用了缓冲机制。接收到数据后，系统立即将数据写入客户端，同时将数据累积到 `updateBuffer` 缓冲区中。当缓冲区达到阈值（100 字符）时，系统才批量更新数据库，这种设计既保证了实时性，又避免了频繁的数据库 I/O 操作对性能的影响。更重要的是，系统实现了客户端写入和数据库保存的错误处理分离，即使客户端断开连接导致写入失败，已接收的内容仍然会被保存到数据库，确保数据的完整性。在流式响应结束后，系统还会保存剩余的缓冲区内容，进一步保证没有数据丢失。

这个过程中面临的主要技术难点包括并发写入的处理、数据一致性的保证以及错误恢复策略。系统通过使用缓冲区机制批量更新而非逐字符更新来解决性能问题，通过分离客户端写入和数据库保存的错误处理来保证数据完整性，通过在流式响应结束后进行最终保存来确保数据一致性。

### RAG（检索增强生成）实现

RAG 功能是系统最重要的技术特性之一，它通过向量检索技术实现了上下文增强，显著提升了 AI 回答的准确性和一致性。整个 RAG 流程从用户查询开始，首先将查询文本转换为向量嵌入（Embedding），然后通过向量相似度搜索在知识库中检索相关的文本片段（chunks），最后将检索到的信息构建成增强的上下文，与用户查询一起发送给大语言模型，从而让模型能够基于更丰富的背景信息生成回答。

文本切片（Chunking）是 RAG 的基础环节，系统实现了智能的切片策略。优先使用段落优先策略，按照段落（`\n\n`）分割文本，这样能够保持语义的完整性。当遇到超长段落时，系统会自动切换到固定大小策略，使用 1000 字符的固定大小进行分割。无论使用哪种策略，系统都会在 chunk 之间保留 200 字符的重叠区域，这样可以避免语义边界丢失的问题。在实现过程中，系统需要准确计算每个 chunk 在原文本中的位置，这涉及到复杂的位置追踪逻辑。系统使用 `textPos` 和 `startOffset` 来准确计算位置，同时处理超长段落和空段落等边界情况，并且通过边界检查机制确保重叠计算不会导致无限循环。

向量嵌入（Embedding）环节使用外部 Embedding API 将文本转换为数值向量。系统支持批量处理多个文本，通过一次 API 调用处理多个 chunks，显著提高了效率。生成的向量以 JSON 格式存储在数据库中，通过 `VectorChunk` 模型的 `EmbeddingJSON` 字段保存，同时保存原始文本内容和元数据信息，包括角色、位置、类型等。

向量检索（Vector Search）使用余弦相似度算法计算查询向量与知识库中所有向量的相似度。余弦相似度通过计算两个向量的点积除以它们的模长乘积得到，值域在 -1 到 1 之间，值越大表示相似度越高。为了提高检索的准确性，系统还实现了时间衰减因子机制，结合内容的创建时间，较新的内容会被赋予更高的权重。具体来说，24 小时内的内容权重为 1.0，之后逐渐降低，30 天后的内容权重为 0.5。最终的相似度分数是语义相似度和时间衰减因子的乘积，这样既保证了语义相关性，又优先使用了最新信息。系统还设置了相似度阈值（0.3），只返回相似度大于阈值的结果，并且在灵感模式下返回最相关的 8 个 chunks。

异步索引机制确保了 RAG 功能不会影响主流程的性能。所有索引操作都在独立的 goroutine 中异步执行，当用户发送消息时，系统立即异步索引用户消息。对于 AI 响应，当内容达到 500 字符时触发索引，流式响应结束后进行最终索引。这种设计的优势在于不阻塞主流程，即使索引失败也不会影响对话功能，并且支持增量索引，只索引新内容，删除旧 chunks 后重新索引。

### 双模式支持

系统设计支持两种工作模式，以适应不同的使用场景。普通对话模式使用 `Conversation` 作为对话容器，使用 `Document` 存储对话中的消息文档。这种模式提供了简单的对话历史管理，支持对话标题的自动生成，文档列表按时间顺序排列，适合日常的对话场景。

灵感模式（长篇故事创作）则使用 `Work` 作为创作容器，相当于一个长篇小说项目，使用 `WorkDocument` 存储创作中的文档片段。这个模式针对长篇故事创作进行了深度优化，首先提供了专门的系统提示，针对长篇故事创作的特点进行了提示词优化，强调一致性、连贯性和风格统一。其次，通过 RAG 上下文增强，系统能够检索相关的人物设定、世界观设定和已有情节，并将这些信息分类组织，构建成结构化的系统提示。系统会根据内容特征自动分类，将检索到的信息分为人物设定、世界观设定、已有情节、用户要求和其他相关信息等类别，然后按优先级组织这些信息，构建详细的系统提示，指导 AI 在创作新内容时严格遵循已有设定，确保新情节与已有情节自然衔接，注意伏笔和线索的呼应，保持文风一致。

### 多模型支持

系统通过 Provider 模式实现了多模型支持，通过 `ChatProvider` 接口抽象了不同模型提供者的实现细节。任何实现了 `ChatProvider` 接口的提供者都可以被系统使用，当前系统支持 OpenAI 兼容接口（如 DeepSeek Chat）和 Anthropic 兼容接口（如 Kimi）。当需要添加新的模型提供者时，只需要在 `services/` 目录下创建新的 provider 文件，实现 `ChatProvider` 接口，并在 `GetProvider` 函数中注册即可，这种设计使得系统具有良好的扩展性。

## 🎯 技术难点与解决方案

在流式响应与数据保存的平衡方面，系统面临的核心挑战是如何在保证实时性的同时确保数据不丢失。流式响应需要实时转发给客户端以提供良好的用户体验，但同时需要保存到数据库以避免数据丢失，而频繁的数据库操作又会影响性能。系统通过缓冲机制解决了这个问题，使用 `updateBuffer` 累积内容，达到阈值（100 字符）才更新数据库，这样既减少了 I/O 操作，又保证了数据的及时保存。同时，系统分离了客户端写入和数据库保存的错误处理，客户端写入失败不会影响数据库保存，并且在流式响应结束后保存剩余缓冲区内容，确保数据完整性。

RAG 索引的性能优化是另一个重要的技术挑战。Embedding API 调用通常比较耗时，大量文本的切片和索引操作也会消耗大量资源，而索引操作不应该阻塞主流程。系统通过异步处理解决了这个问题，所有索引操作都在 goroutine 中异步执行，这样主流程不会被阻塞。同时，系统实现了批量 Embedding 机制，一次 API 调用处理多个 chunks，提高了效率。系统还实现了增量索引，只索引新内容，删除旧 chunks 后重新索引，避免了重复工作。此外，系统设置了阈值触发机制，内容达到一定长度（500 字符）才触发索引，避免了不必要的索引操作。

文本切片的准确性是实现 RAG 功能的基础，系统需要准确计算每个 chunk 在原文本中的位置，处理超长段落和边界情况，并且确保重叠机制不会导致无限循环。系统通过位置追踪机制解决了位置计算问题，使用 `textPos` 和 `startOffset` 准确计算位置。通过边界检查防止 `nextTextPos` 倒退或越界，避免了无限循环的问题。当段落切片失败时，系统会自动回退到固定大小切片，确保总是能够成功切片。

向量检索的准确性直接影响 RAG 功能的效果。简单的余弦相似度可能不够准确，需要结合时间因素优先使用最新内容，并且需要合理选择相似度阈值。系统通过时间衰减机制解决了时间因素的问题，结合创建时间，较新内容权重更高。通过阈值过滤，只返回相似度大于 0.3 的结果，保证了检索质量。在灵感模式下，系统返回最相关的 8 个 chunks，这个数量经过优化，既保证了信息的丰富性，又避免了上下文过长的问题。

数据一致性和隔离是多用户系统的基础要求。系统需要实现多用户数据隔离，管理对话和创作的关联关系，并且保证文档更新的原子性。系统通过用户 ID 验证实现了数据隔离，所有查询都包含 `userID` 条件，确保用户只能访问自己的数据。通过事务处理保证了关键操作的原子性，使用 GORM 的 Preload 预加载关联数据，提高了查询效率。

## 🚀 快速开始

系统要求 Go 1.21 或更高版本，数据库使用 SQLite 3，通过 GORM 自动管理，无需额外安装。安装过程非常简单，首先进入后端目录，然后运行 `go mod download` 安装依赖。接下来需要创建 `.env` 文件配置环境变量，包括服务器端口（默认 8080）、数据库路径（默认 grandma.db）、OpenAI 和 Anthropic 的 API Key 和 Base URL，以及 RAG 相关的配置，包括是否启用 RAG、Embedding 模型名称、Embedding API 的 Base URL 和 API Key。配置完成后，运行 `go run main.go` 即可启动服务器，服务器将在 `http://localhost:8080` 启动。可以通过访问健康检查接口 `http://localhost:8080/health` 验证安装是否成功，正常情况应该返回 `{"status":"ok"}`。

## 📚 API 文档

系统提供了完整的 RESTful API 接口。聊天接口 `POST /api/chat` 用于发送聊天请求并获取流式响应，请求体需要包含用户 ID、模型名称、可选的对话 ID（普通模式）或创作 ID（灵感模式），以及消息数组。响应是流式文本响应（`text/event-stream`），实时返回 AI 生成的内容，在流式响应末尾会包含元数据，格式为 `<metadata>{"document_id":"doc_xxx"}</metadata>`。

对话管理接口包括获取对话列表、创建新对话、获取指定对话详情、更新对话标题和删除对话等功能。获取对话列表支持分页，可以通过 `page` 和 `page_size` 参数控制。文档管理接口提供了获取文档列表、获取指定文档、更新文档和删除文档等功能，获取文档列表支持翻页，可以通过 `before_id` 和 `limit` 参数控制。

创作管理接口（灵感模式）提供了获取创作列表、创建新创作、获取创作的所有文档、创建新文档、更新文档内容和标题、删除文档等功能。模型列表接口 `GET /api/models` 返回系统支持的所有模型列表，包括模型 ID、名称和提供者信息。

## ⚙️ 配置说明

系统通过环境变量进行配置，所有配置项都有合理的默认值。服务器端口默认为 8080，数据库路径默认为 grandma.db。OpenAI 和 Anthropic 的配置包括 API Key 和 Base URL，如果使用对应的模型，则需要配置相应的 API Key。RAG 功能可以通过 `ENABLE_RAG` 环境变量启用或禁用，默认为启用。如果启用 RAG，需要配置 Embedding 相关的参数，包括模型名称（默认 text-embedding-v4）、Base URL 和 API Key。

在代码层面，RAG 的切片参数可以调整，包括每个 chunk 的最大字符数（默认 1000）、chunk 之间的重叠字符数（默认 200）和最小 chunk 大小（默认 100）。检索参数也可以调整，包括返回最相关的 chunks 数量（灵感模式默认 8）和相似度阈值（默认 0.3）。这些参数可以根据实际使用场景进行调整，以优化 RAG 的效果。

## 💻 开发指南

系统采用清晰的项目结构，按照功能域进行组织。配置管理模块位于 `config/` 目录，负责配置加载和解析。数据库模块位于 `database/` 目录，处理数据库初始化和迁移。数据模型模块位于 `models/` 目录，定义了所有业务实体和请求响应结构。数据访问层位于 `repository/` 目录，每个业务实体都有对应的 Repository。基础服务层位于 `services/` 目录，提供了模型提供者、Embedding、文本切片和向量存储等通用服务。业务模块层位于 `modules/` 目录，包含了所有业务逻辑。工具函数模块位于 `utils/` 目录，提供了通用工具函数。

添加新功能非常简单。要添加新的模型提供者，只需要在 `services/` 目录下创建新的 provider 文件，实现 `ChatProvider` 接口，并在 `services/provider.go` 的 `GetProvider` 函数中注册即可。要添加新的业务模块，需要在 `modules/` 目录下创建新模块，实现 `Handler` 和 `Service`，然后在 `main.go` 中注册路由。要扩展 RAG 功能，可以修改 `modules/rag/rag_service.go` 调整 RAG 服务逻辑，修改 `services/chunking.go` 调整切片策略，修改 `services/vector_store.go` 优化检索算法。

系统遵循严格的代码规范，所有导出的结构体和函数都需要有注释，注释格式为 `// XXXX 注释内容`。代码使用 Go 标准代码风格，错误处理要明确，避免静默失败，异步操作要有错误日志。

在测试方面，建议为 Service 层和 Repository 层编写单元测试，测试 API 接口的完整流程，测试 RAG 索引和检索的性能，以及测试流式响应的并发处理能力。

在性能优化方面，数据库优化包括为常用查询字段添加索引，使用连接池管理数据库连接，在生产环境考虑使用 PostgreSQL 替代 SQLite。RAG 优化包括使用专业的向量数据库（如 Milvus、Pinecone），实现向量索引（如 HNSW），批量处理 Embedding 请求。缓存策略包括缓存常用的对话历史，缓存 Embedding 结果，使用 Redis 缓存热点数据。

## 📝 许可证

本项目采用 MIT 许可证。

## 🤝 贡献

欢迎提交 Issue 和 Pull Request！

---

**注意**：本项目仍在积极开发中，API 可能会有变化。建议在生产环境使用前进行充分测试。
